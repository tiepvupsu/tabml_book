
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Word2vec &#8212; Machine Learning cho dữ liệu dạng bảng</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/my.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="canonical" href="https://machinelearningcoban.com/tabml_book/ch_embedding/word2vec.html" />
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Instacart Product2vec" href="product2vec.html" />
    <link rel="prev" title="Embedding" href="embedding.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning cho dữ liệu dạng bảng</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Lời nói đầu
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Giới thiệu
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/properties.html">
   Đặc điểm của dữ liệu dạng bảng
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/pipeline.html">
   Machine Learning pipeline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/why_pipeline.html">
   Tại sao cần xây dựng pipeline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/tabml.html">
   Thư viện tabml đi kèm cuốn sách
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/titanic_pipeline.html">
   Pipeline đơn giản cho cuộc thi Titanic
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/main_contents.html">
   Bố cục cuốn sách
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/contributions.html">
   Đóng góp vào dự án
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/datasets.html">
   Các bộ dữ liệu sử dụng trong sách
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Kỹ thuật xử lý dữ liệu
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_data_processing/eda.html">
   Phân tích Khám phá Dữ liệu - EDA
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/eda_purpose.html">
     Mục đích của EDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/eda_titanic.html">
     EDA cho dữ liệu Titanic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/eda_cali_housing.html">
     EDA cho dữ liệu California Housing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/pandas-profiling.html">
     Pandas profiling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_data_processing/data_cleaning.html">
   Làm sạch dữ liệu
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/process_outliers.html">
     Xử lý các giá trị ngoại lệ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/process_missing.html">
     Xử lý dữ liệu bị khuyết
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_data_processing/categorical_data.html">
   Đặc trưng hạng mục (WIP)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/onehot.html">
     Mã hóa one-hot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/hashing.html">
     Hashing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_data_processing/crossing.html">
     Crossing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_data_processing/numeric_data.html">
   Đặc trưng dạng số (WIP)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Embedding
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="embedding.html">
   Embedding
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="product2vec.html">
   Instacart Product2vec
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Hệ thống gợi ý
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_recommendation_system/introduction.html">
   Hệ thống gợi ý
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_recommendation_system/ml_1m.html">
   Bộ dữ liệu MovieLens-1M
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_recommendation_system/content_based.html">
   Hệ thống dựa trên nội dung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_recommendation_system/matrix_factorization.html">
   Matrix Factorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_recommendation_system/factorization_machine.html">
   Factorization machine
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Đóng góp từ tác giả khác
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_data_processing/timeseries_data.html">
   Dữ liệu chuỗi thời gian
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_model/decision_tree.html">
   Decision Tree algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_model/random_forest.html">
   Random Forest algorithm
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Phụ lục
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ap/visualization.html">
   Minh họa dữ liệu
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/ch_embedding/word2vec.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/ch_embedding/word2vec.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/tiepvupsu/tabml_book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/tiepvupsu/tabml_book/issues/new?title=Issue%20on%20page%20%2Fch_embedding/word2vec.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/tiepvupsu/tabml_book/edit/main/book/ch_embedding/word2vec.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/tiepvupsu/tabml_book/main?urlpath=tree/book/ch_embedding/word2vec.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gioi-thieu">
   Giới thiệu
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mot-vai-dinh-nghia">
   Một vài định nghĩa
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#skip-gram">
   Skip-gram
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xay-dung-ham-mat-mat">
     Xây dựng hàm mất mát
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bieu-dien-duoi-dang-mang-neural">
     Biểu diễn dưới dạng mạng neural
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#toi-uu-ham-mat-mat">
     Tối ưu hàm mất mát
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xap-xi-ham-mat-mat-va-lay-mau-am">
     Xấp xỉ hàm mất mát và Lấy mẫu âm
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#continous-bag-of-words-cbow">
   Continous Bag of Words (CBOW)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#thao-luan">
   Thảo luận
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tai-lieu-tham-khao">
   Tài liệu tham khảo
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="word2vec">
<span id="sec-word2vec"></span><h1>Word2vec<a class="headerlink" href="#word2vec" title="Permalink to this headline">¶</a></h1>
<div class="section" id="gioi-thieu">
<h2>Giới thiệu<a class="headerlink" href="#gioi-thieu" title="Permalink to this headline">¶</a></h2>
<p>Word2vec là một mô hình đơn giản và nổi tiếng giúp tạo ra các biểu diễn embedding của từ trong một không gian có số chiều thấp hơn nhiều lần so với số từ trong từ điển. Ý tưởng của word2vec đã được sử dụng trong nhiều bài toán với dữ liệu khác xa với dữ liệu ngôn ngữ. Trong cuốn sách này, ý tưởng của word2vec sẽ được trình bày và một ví dụ minh họa ứng dụng word2vec để tạo một mô hình <em>product2vec</em> giúp tạo ra các embedding khác nhau cho thực phẩm và đồ gia dụng.</p>
<p>Ý tưởng cơ bản của word2vec có thể được gói gọn trong các ý sau:</p>
<ul class="simple">
<li><p>Hai từ xuất hiện trong những văn cảnh giống nhau thường có ý nghĩa gần với nhau.</p></li>
<li><p>Ta có thể đoán được một từ nếu biết các từ xung quanh nó trong câu. Ví dụ, với câu “Hà Nội là … của Việt Nam” thì từ trong dấu ba chấm khả năng cao là “thủ đô”. Với câu hoàn chỉnh “Hà Nội là thủ đô của Việt Nam”, mô hình word2vec sẽ xây dựng ra embeding của các từ sao cho xác suất để từ trong dấu ba chấm là “thủ đô” là cao nhất.</p></li>
</ul>
</div>
<div class="section" id="mot-vai-dinh-nghia">
<h2>Một vài định nghĩa<a class="headerlink" href="#mot-vai-dinh-nghia" title="Permalink to this headline">¶</a></h2>
<p>Trong ví dụ trên đây, từ “thủ đô” đang được xét và được gọi là <em>target word</em> hay <em>từ đích</em>. Những từ xung quanh nó được gọi là <em>context words</em> hay <em>từ ngữ cảnh</em>. Với mỗi từ đích trong một câu của cơ sở dữ liệu, các từ ngữ cảnh được định nghĩa là các từ trong cùng câu có vị trí cách từ đích một khoảng không quá <span class="math notranslate nohighlight">\(C/2\)</span> với <span class="math notranslate nohighlight">\(C\)</span> là một số tự nhiên dương. Như vậy, với mỗi từ đích, ta sẽ có một bộ không quá <span class="math notranslate nohighlight">\(C\)</span> từ ngữ cảnh.</p>
<p>Xét ví dụ sau đây với câu tiếng Anh: “The quick brown fox jump over the lazy dog” với <span class="math notranslate nohighlight">\(C = 4\)</span>.</p>
<!--
![](http://mccormickml.com/assets/word2vec/training_data.png)
-->
<div class="figure align-default" id="img-word2vec-training-data">
<img alt="../_images/word2vec_training_data.png" src="../_images/word2vec_training_data.png" />
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Ví dụ về các cặp (từ đích, từ ngữ cảnh) (Nguồn: <a class="reference external" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a>).</span><a class="headerlink" href="#img-word2vec-training-data" title="Permalink to this image">¶</a></p>
</div>
<p>Khi “the” là từ đích, ta có cặp dữ liệu huấn luyện là (the, quick) và (the, brown). Khi “brown” là từ đích, ta có cặp dữ liệu huấn luyện là (brown, the), (brown, quick), (brown, fox) và (brown, jumps).</p>
<p>Word2vec định nghĩa hai embedding vector cùng chiều cho mỗi từ <span class="math notranslate nohighlight">\(w\)</span> trong từ điển. Khi nó là một từ đích, embedding vector của nó là <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>; khi nó là một từ ngữ cảnh, embedding của nó là <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>. Sở dĩ ta cần hai embedding khác nhau vì ý nghĩa của từ đó khi nó là từ đích và từ ngữ cảnh là khác nhau. Tương ứng với đó, ta có hai ma trận embedding <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> và <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> cho các từ đích và các từ ngữ cảnh.</p>
<p>Có hai cách khác nhau xây dựng mô hình word2vec:</p>
<ul class="simple">
<li><p>Skip-gram: Dự đoán những từ ngữ cảnh nếu biết trước từ đích.</p></li>
<li><p>CBOW (Continuous Bag of Words): Dựa vào những từ ngữ cảnh để dự đoán từ đích.</p></li>
</ul>
<p>Mỗi cách có những ưu nhược điểm khác nhau và áp dụng với những loại dữ liệu khác nhau.</p>
</div>
<div class="section" id="skip-gram">
<h2>Skip-gram<a class="headerlink" href="#skip-gram" title="Permalink to this headline">¶</a></h2>
<div class="section" id="xay-dung-ham-mat-mat">
<h3>Xây dựng hàm mất mát<a class="headerlink" href="#xay-dung-ham-mat-mat" title="Permalink to this headline">¶</a></h3>
<p>Mọi tính toán trong mục này được xây dựng xung quanh một từ ngữ cảnh. Hàm mất mát tổng cộng sẽ là tổng của hàm mất mát tại mỗi từ ngữ cảnh. Việc tối ưu hàm mất mát có thể được thực hiện thông qua <a class="reference external" href="https://machinelearningcoban.com/2017/01/12/gradientdescent/">Gradient Descent</a> trên từng từ ngữ cảnh hoặc một batch các từ ngữ cảnh.</p>
<p>Xét ví dụ bên trên với từ đích là “fox” và các từ ngữ cảnh là “quick”, “brown”, “jumps” và “over”. Việc dự đoán xác suất xảy ra các từ ngữ cảnh khi biết từ đích được mô hình hóa bởi:</p>
<div class="math notranslate nohighlight">
\[P(&quot;\textrm{quick}&quot;, &quot;\textrm{brown}&quot;, &quot;\textrm{jumps}&quot;, &quot;\textrm{over}&quot; | &quot;\textrm{fox}&quot;)\]</div>
<p>Ta có thể giả sử rằng sự xuất hiện của một từ ngữ cảnh khi biết từ đích <em>độc lập</em> với các từ ngữ cảnh khác để xấp xỉ xác suất trên đây bởi:</p>
<div class="math notranslate nohighlight">
\[P(&quot;\textrm{quick}&quot;|&quot;\textrm{fox}&quot;) P(&quot;\textrm{brown}&quot; |&quot;\textrm{fox}&quot;) P(&quot;\textrm{jumps}&quot;|&quot;\textrm{fox}&quot;) P(&quot;\textrm{over}&quot;|&quot;\textrm{fox}&quot;)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Giả sử về việc các từ ngữ cảnh xuất hiện độc lập với nhau xunh quanh từ đích mâu thuẫn với ý tưởng của word2vec là những từ trong cùng văn cảnh có liên quan đến nhau. Tuy nhiên, giả thiết này giúp mô hình và độ phức tạp giảm đi rất nhiều trong khi vẫn mang lại kết quả khả quan.</p>
</div>
<p>Giả sử từ đích có chỉ số <span class="math notranslate nohighlight">\(t\)</span> trong từ điển <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> và tập hợp các chỉ số của các từ ngữ cảnh tương ứng là <span class="math notranslate nohighlight">\(\mathcal{C}_t\)</span>.
Số lượng phần tử của <span class="math notranslate nohighlight">\(\mathcal{C}_t\)</span> dao động từ <span class="math notranslate nohighlight">\(C/2\)</span> (nếu <span class="math notranslate nohighlight">\(w_t\)</span> đứng đầu hoặc cuối câu) tới <span class="math notranslate nohighlight">\(C\)</span> (nếu <span class="math notranslate nohighlight">\(w_t\)</span> đứng ở giữa câu và có đủ <span class="math notranslate nohighlight">\(C/2\)</span> từ ngữ cảnh ở mỗi phía).</p>
<!--
Giả sử từ đích có chỉ số $t$ trong từ điển $\mathcal{V}$ và tập các từ ngữ cảnh là $\mathcal{C} = \{w_{c_1}, w_{c_2}, \dots\}$.


Một cách tổng quát, giả sử từ đích là $w_t$ và các từ ngữ cảnh là $w_1, w_2, \dots, w_C$ (số lượng từ ngữ cảnh ứng với một từ đích có thể nhỏ hơn C nhưng biểu diễn toán học sẽ không có nhiều khác biệt).

-->
<p>Từ dữ liệu đã có, ta cần một mô hình sao cho xác suất dưới đây càng lớn càng tốt với mỗi từ ngữ cảnh <span class="math notranslate nohighlight">\(w_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\prod_{c \in \mathcal{C}_t}P(w_c|w_t)
\]</div>
<p>Để tránh các sai số tính toán khi nhân các số nhỏ hơn 1 với nhau, bài toán tối ưu này thường được đưa về bài toán tối thiểu đối số của log (thường được gọi là <em>negative log loss</em>):</p>
<div class="math notranslate nohighlight">
\[
-\sum_{c \in \mathcal{C}_t}\log P(w_c|w_t)
\]</div>
<p>Xác suất có điều kiện <span class="math notranslate nohighlight">\(P(w_c|w_t)\)</span> được định nghĩa bởi:</p>
<div class="math notranslate nohighlight" id="equation-word2vec-softmax">
<span class="eqno">(1)<a class="headerlink" href="#equation-word2vec-softmax" title="Permalink to this equation">¶</a></span>\[
P(w_c | w_t) = \frac{\exp(\mathbf{u}_t^T\mathbf{v}_c)}{\sum_{i=1}^{N}\exp(\mathbf{u}_t^T\mathbf{v}_i)}
\]</div>
<p>với <span class="math notranslate nohighlight">\(N\)</span> là số phần tử của từ điển <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>. Ở đây <span class="math notranslate nohighlight">\(\exp(\mathbf{u}_t^T\mathbf{v}_c)\)</span> thể hiện mỗi quan hệ giữa từ đích <span class="math notranslate nohighlight">\(w_t\)</span> và từ ngữ cảnh <span class="math notranslate nohighlight">\(w_c\)</span>. Biểu thức này càng cao thì xác suất thu được càng lớn. Tích vô hướng <span class="math notranslate nohighlight">\(\mathbf{u}_t^T\mathbf{v}_c\)</span> cũng thể hiện sự tương tự giữa hai vector.</p>
<p>Biểu thức này rất giống với công thức <a class="reference external" href="https://machinelearningcoban.com/2017/02/17/softmax/">Softmax</a>. Việc định nghĩa xác suất như biểu thức <a class="reference internal" href="#equation-word2vec-softmax">(1)</a> ở trên đảm bảo rằng</p>
<div class="math notranslate nohighlight">
\[
\sum_{w \in \mathcal{V}} P(w | w_t) = 1
\]</div>
<p>Tóm lại, hàm mất mát ứng với từ đích <span class="math notranslate nohighlight">\(w_t\)</span> theo <span class="math notranslate nohighlight">\(\mathbf{U}, \mathbf{V}\)</span> được cho bởi</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\mathbf{U}, \mathbf{V}; w_t) = -\sum_{c \in \mathcal{C}_t} \log \frac{\exp(\mathbf{u}_t^T\mathbf{v}_c)}{\sum_{i=1}^{N}\exp(\mathbf{u}_t^T\mathbf{v}_i)}
\]</div>
</div>
<div class="section" id="bieu-dien-duoi-dang-mang-neural">
<h3>Biểu diễn dưới dạng mạng neural<a class="headerlink" href="#bieu-dien-duoi-dang-mang-neural" title="Permalink to this headline">¶</a></h3>
<p>Ta có thể thấy:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>skip-gram word2vec là một mạng neural vô cùng đơn giản với chỉ một tầng ẩn không có hàm kích hoạt.</p>
</div>
<!--
![](imgs/word2vec1.png)
-->
<div class="figure align-default" id="img-word2vec-skipgram">
<img alt="../_images/word2vec1.png" src="../_images/word2vec1.png" />
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Minh họa Skip-gram dưới dạng mạng neural.</span><a class="headerlink" href="#img-word2vec-skipgram" title="Permalink to this image">¶</a></p>
</div>
<p>Nhận xét này có thể được minh họa trên <a class="reference internal" href="#img-word2vec-skipgram"><span class="std std-numref">Fig. 11</span></a>. Ở đây, <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> chính là kết quả của phép nhân vector one-hot tương ứng với <span class="math notranslate nohighlight">\(w_t\)</span> với ma trận trọng số <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>, vì vậy đây chính là giá trị đầu ra của của tầng ẩn ở giữa khi xét từ đích <span class="math notranslate nohighlight">\(w_t\)</span>. Tiếp theo, đầu ra của tầng ẩn không hàm kích hoạt này được nhân trực tiếp với ma trận trọng số đầu ra <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> để được <span class="math notranslate nohighlight">\(\mathbf{u}_t^T\mathbf{V}\)</span>, đây chính là giá trị vector logit trước khi đi vào hàm kích hoạt softmax như trong biểu thức <a class="reference internal" href="#equation-word2vec-softmax">(1)</a>.</p>
<p>Kiến trúc đơn giản này giúp word2vec hoạt động tốt ngay cả khi số lượng từ trong từ điển là cực lớn (có thể lên tới nhiều triệu từ). Lưu ý rằng kích thước đầu vào và đầu ra của mạng word2vec này bằng với số lượng từ trong từ điển.</p>
</div>
<div class="section" id="toi-uu-ham-mat-mat">
<h3>Tối ưu hàm mất mát<a class="headerlink" href="#toi-uu-ham-mat-mat" title="Permalink to this headline">¶</a></h3>
<p>Việc tối ưu hai ma trận trọng số <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> và <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> được thực hiện thông qua các thuật toán <a class="reference external" href="https://machinelearningcoban.com/2017/01/12/gradientdescent/">Gradient Descent</a>. Các thuật toán tối ưu dạng này yêu cầu tính gradient cho từng ma trận.</p>
<p>Xét riêng số hạng</p>
<div class="math notranslate nohighlight">
\[
\log P(w_c | w_t) = \log\left(\frac{\exp(\mathbf{u}_t^T\mathbf{v}_c)}{\sum_{i=1}^{N}\exp(\mathbf{u}_t^T\mathbf{v}_i)}\right) = \mathbf{u}_t^T \mathbf{v_c} - \log \left(\sum_{i=1}^{N}\exp(\mathbf{u}_t^T\mathbf{v}_i)\right)
\]</div>
<!--
Đạo hàm theo $\mathbf{v}_c$:
$$
\frac{\partial \log P(w_c | w_t)}{\partial \mathbf{v_c}} = \mathbf{u}_t -
\frac{\exp(\mathbf{u}_t^T\mathbf{v}_c) \mathbf{u}_t}{\sum_{i=1}^{N}\exp(\mathbf{u}_t^T\mathbf{v}_i)} = \mathbf{u}_t\left(1 - P(w_c | w_t)\right)
$$

-->
<p>Đạo hàm theo <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \log P(w_c | w_t)}{\partial \mathbf{u_t}} = \mathbf{v}_c -
\sum_{j=1}^N \left(\frac{\exp(\mathbf{u}_t^T\mathbf{v}_j) \mathbf{v}_j}{\sum_{i=1}^{N}\exp(\mathbf{u}_t^T\mathbf{v}_i)}\right) = \mathbf{v}_c - \sum_{j=1}^N P(w_j | w_t) \mathbf{v}_j
\]</div>
<p>Như vậy, mặc dù gradient này rất đẹp, chúng ta vẫn cần phải tính toán các xác suất <span class="math notranslate nohighlight">\(P(w_j | w_t)\)</span>. Mỗi xác suất này phụ thuộc toàn bộ ma trận trọng số <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> và vector <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span>. Như vậy ta cần cập nhập tổng cộng <span class="math notranslate nohighlight">\(N*d + d\)</span> trọng số. Đây rõ ràng là một con số rất lớn với <span class="math notranslate nohighlight">\(N\)</span> lớn.</p>
</div>
<div class="section" id="xap-xi-ham-mat-mat-va-lay-mau-am">
<h3>Xấp xỉ hàm mất mát và Lấy mẫu âm<a class="headerlink" href="#xap-xi-ham-mat-mat-va-lay-mau-am" title="Permalink to this headline">¶</a></h3>
<p>Để tránh việc cập nhật rất nhiều tham số này trong một lượt, một phương pháp xấp xỉ được đề xuất giúp cải thiện tốc độ tính toán đáng kể. Mỗi xác suất <span class="math notranslate nohighlight">\(P(w_c | w_t)\)</span> được mô hình bởi một hàm <a class="reference external" href="https://machinelearningcoban.com/2017/01/27/logisticregression/#sigmoid-function">sigmoid</a> thay vì hàm softmax:</p>
<div class="math notranslate nohighlight">
\[
P(w_c | w_t) = \frac{1}{1 + \exp(-\mathbf{u}_t^T \mathbf{v}_c)}
\]</div>
<p>Lưu ý rằng tổng các xác suất <span class="math notranslate nohighlight">\(\sum_{w_c \in \mathbf{V}} P(w_c | w_t)\)</span> không còn bằng 1 nữa. Tuy nhiên, nó vẫn mang ý nghĩa về xác suất có mặt của riêng từ ngữ cảnh <span class="math notranslate nohighlight">\(w_c\)</span> đi cùng với từ đích <span class="math notranslate nohighlight">\(w_t\)</span>.</p>
<p>Lúc này, việc tính toán <span class="math notranslate nohighlight">\(P(w_c | w_t)\)</span> chỉ còn phụ thuộc vào vector <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> và vector <span class="math notranslate nohighlight">\(\mathbf{v}_c\)</span> (thay vì cả ma trận <span class="math notranslate nohighlight">\(\mathbf{V}\)</span>). Tương ứng với số hạng này, sẽ chỉ có <span class="math notranslate nohighlight">\(2d\)</span> trọng số cần được cập nhật cho mỗi cặp <span class="math notranslate nohighlight">\((w_t, w_c)\)</span>. Số lượng trọng số này <em>không</em> phụ thuộc vào kích thước từ điển, khiến cho cách mô hình này có thể hoạt động tốt với <span class="math notranslate nohighlight">\(N\)</span> rất lớn.</p>
<p>Có một vấn đề lớn với cách mô hình hóa này!</p>
<p>Vì không có sự ràng buộc giữa các xác suất <span class="math notranslate nohighlight">\(P(w_c | w_t)\)</span>, khi cố gắng tối đa hóa mỗi xác suất sẽ dẫn đến việc nghiệm thu được thỏa mãn mọi <span class="math notranslate nohighlight">\(P(w_c | w_t)\)</span> đều cao. Điều này sẽ đạt được khi <span class="math notranslate nohighlight">\(\exp(-\mathbf{u}_t^T \mathbf{v}_c)\)</span> xấp xỉ 0. Chỉ cần toàn bộ các phần tử của <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> và <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> tiến tới dương vô cùng là thỏa mãn. Việc xấp xỉ này bây giờ trở nên tầm thường và vô nghĩa. Để tránh vấn đề này, ta cần thêm đưa thêm các ràng buộc sao cho tồn tại các xác suất <span class="math notranslate nohighlight">\(P(w_n | w_t)\)</span> khác cần được tối thiểu hóa khi xét tới từ đích <span class="math notranslate nohighlight">\(w_t\)</span>.</p>
<p>Bản chất của bài toán tối ưu ban đầu là xây dựng mô hình sao cho với mỗi từ đích, xác suất của một từ ngữ cảnh xảy ra là cao trong khi xác suất của <em>toàn bộ</em> các từ ngoài ngữ cảnh đó là thấp – việc này được thể hiện trong hàm softmax. Để hạn chế tính toán, trong phương pháp này ta chỉ lấy mẫu ngẫu nhiên một vài từ ngoài ngữ cảnh đó để tối ưu. Các từ trong ngữ cảnh được gọi là “từ dương”, các từ ngoài ngữ cảnh được gọi là “từ âm”; vì vậy phương pháp này còn có tên gọi khác là “lấy mẫu âm” (<em>negative sampling</em>).</p>
<p>Khi đó, với mỗi từ đích, ta có một bộ các từ ngữ cảnh với nhãn là 1 và 0 tương ứng với các từ ngữ cảnh ban đầu (gọi là <em>ngữ cảnh dương</em>) và các từ <em>ngữ cảnh âm</em> được lấy mẫu từ ngoài tập ngữ cảnh dương đó. Với các từ ngữ cảnh dương, <span class="math notranslate nohighlight">\(-\log(P(w_c | w_t))\)</span> tương tự với <a class="reference external" href="https://machinelearningcoban.com/2017/01/27/logisticregression/#-ham-mat-mat-va-phuong-phap-toi-uu">hàm mất mát trong hồi quy logistic</a> với nhãn bằng 1. Tương tự, ta có thể dùng <span class="math notranslate nohighlight">\(-\log(1 - P(w_c | w_t))\)</span> như là hàm mất mát cho các từ ngữ cảnh âm với nhãn bằng 0.</p>
</div>
</div>
<div class="section" id="continous-bag-of-words-cbow">
<h2>Continous Bag of Words (CBOW)<a class="headerlink" href="#continous-bag-of-words-cbow" title="Permalink to this headline">¶</a></h2>
<p>Ngược với Skip-gram, Continous bag of Words đi tìm xác suất xảy ra từ đích khi biết các từ ngữ cảnh xung quanh. Ta cần mô hình hóa dữ liệu sao cho xác suất sau đây đạt giá trị lớn:</p>
<div class="math notranslate nohighlight">
\[P(&quot;\textrm{fox}&quot; | &quot;\textrm{quick}&quot;, &quot;\textrm{brown}&quot;, &quot;\textrm{jumps}&quot;, &quot;\textrm{over}&quot;)\]</div>
<p>Vì có nhiều từ ngữ cảnh trong điều kiện, chúng thường được đơn giản hóa bằng cách lấy một từ “trung bình” làm đại diện.</p>
<div class="math notranslate nohighlight">
\[
P(w_t | \bar{w}_{\mathcal{C}_t})
\]</div>
<p>với <span class="math notranslate nohighlight">\(\bar{w}_{\mathcal{C}_t}\)</span> là trung bình cộng của các từ trong ngữ cảnh của từ đích <span class="math notranslate nohighlight">\(w_t\)</span>. Embedding của từ trung bình này là trung bình của embedding các từ ngữ cảnh. Xác xuất này cũng được định nghĩa tương tự như trong Skip-gram:</p>
<div class="math notranslate nohighlight">
\[
P(w_t | \bar{w}_{\mathcal{C}_t}) = \frac{\exp\left(\mathbf{u}_t^T\frac{1}{C}\sum_{c \in \mathcal{C}_t}\mathbf{v}_c\right)}{\sum_{i=1}^N\exp\left(\mathbf{u}_i^T\frac{1}{C}\sum_{c \in \mathcal{C}_t}\mathbf{v}_c\right)}
\]</div>
<p>Biểu diễn mạng neural cho CBOW được thể hiện như trong <a class="reference internal" href="#img-word2vec-cbow"><span class="std std-numref">Fig. 12</span></a> dưới đây:</p>
<!--
![](imgs/word2vec2.png)
-->
<div class="figure align-default" id="img-word2vec-cbow">
<img alt="../_images/word2vec2.png" src="../_images/word2vec2.png" />
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">Minh họa CBOW dưới dạng mạng neural.</span><a class="headerlink" href="#img-word2vec-cbow" title="Permalink to this image">¶</a></p>
</div>
<p>Lưu ý rằng giá trị tại tầng ẩn là trung bình cộng của các embedding của các từ ngữ cảnh.</p>
<p>Kỹ thuật tối ưu likelihood này cũng tương tự như trong Skip-gram và phương pháp lấy mẫu âm với các từ đích cũng có thể được sử dụng một cách tương tự.</p>
<p>Câu hỏi: Sau khi huấn luyện mô hình xong, ta sẽ lấy ma trận nào làm embedding cho các từ?</p>
</div>
<div class="section" id="thao-luan">
<h2>Thảo luận<a class="headerlink" href="#thao-luan" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Word2vec là một phương pháp xây dựng embedding cho các từ trong từ điển dựa vào các ngữ cảnh trong câu.</p></li>
<li><p>Với mỗi ngữ cảnh, ta định nghĩa “từ đích” là từ trung tâm, “từ ngữ cảnh” là các từ xung quanh nó.</p></li>
<li><p>Có hai cách mô hình hóa dữ liệu là Skip-gram và CBOW. Skip-gram giả sử rằng từ một từ đích ta có thể suy ra các từ ngữ cảnh. CBOW giả sử rằng từ các từ ngữ cảnh, ta có thể đoán được từ đích.</p></li>
<li><p>Skip-gram làm việc tốt với dữ liệu nhỏ, nó có khả năng biểu diễn tốt những từ có tần suất thấp. Việc này hợp lý vì ta xây dựng được nhiều mẫu huấn luyện xung quanh từ có tần suất thấp này.</p></li>
<li><p>CBOW phù hợp với các bộ dữ liệu lớn khi mà số mẫu huấn luyện được tạo ra từ mỗi ngữ cảnh (chỉ là một) ít hơn nhiều so với Skip-gram (tỉ lệ với kích thước cửa số ngữ cảnh). CBOW biểu diễn tốt hơn các từ xảy ra thường xuyên.</p></li>
<li><p>Word2vec không chỉ có thể sử dụng để tạo embedding cho các từ mà còn có thể áp dụng cho các bộ dữ liệu khác mà sự xuất hiện của một đối tượng phụ thuộc vào các đối tượng khác trong cùng văn cảnh. Trong bài <a class="reference external" href="https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484">Using Word2vec for Music Recommendations</a>, tác giả coi mỗi một lượt nghe nhạc của người dùng là một “câu” và mỗi bài nhạc là một “từ”. Từ đó xây dựng được các embedding cho các bài hát và gợi ý những bài hát mà người dùng có khả năng thích nghe.
Trong phần tiếp theo, chúng ta sẽ sử dụng Skip-gram Word2vec để xây dựng embedding cho các <strong>sản phẩm</strong> trong <a class="reference external" href="https://www.kaggle.com/c/instacart-market-basket-analysis">bộ dữ liệu Instacart</a>.</p></li>
<li><p>Ngoài lấy mẫu âm, <a class="reference external" href="http://d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html#hierarchical-softmax">softmax phân tầng</a> cũng là một phương pháp làm giảm độ phức tạp khi tối ưu hàm mất mát cho word2vec.</p></li>
</ul>
</div>
<div class="section" id="tai-lieu-tham-khao">
<h2>Tài liệu tham khảo<a class="headerlink" href="#tai-lieu-tham-khao" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Word2vec paper</a></p>
<p><a class="reference external" href="https://www.tensorflow.org/tutorials/text/word2vec">Word2vec tensorflow</a></p>
<p><a class="reference external" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">Word2Vec Tutorial Part 2 - Negative Sampling</a></p>
<p><a class="reference external" href="http://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html">Word Embedding (word2vec), Dive into Deep Learning</a></p>
<p><a class="reference external" href="https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484">Using Word2vec for Music Recommendations</a></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch_embedding"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="embedding.html" title="previous page">Embedding</a>
    <a class='right-next' id="next-link" href="product2vec.html" title="next page">Instacart Product2vec</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Tiep Vu<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            <div id="disqus_thread"></div>
  <script>
      /**
      *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
      *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
      /*
      var disqus_config = function () {
      this.page.url = machinelearningcoban.com;  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = tabml; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
      };
      */
      (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://tabml.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-89509207-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>